{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('/Users/maraqawalid/scaling_to_production/05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (32561, 14)\n",
      "Shape of Y: (32561,)\n",
      "100    1\n",
      "101    1\n",
      "102    0\n",
      "103    0\n",
      "104    0\n",
      "105    1\n",
      "106    0\n",
      "107    0\n",
      "108    0\n",
      "109    0\n",
      "110    0\n",
      "111    1\n",
      "112    1\n",
      "113    0\n",
      "114    0\n",
      "115    0\n",
      "116    0\n",
      "117    1\n",
      "118    0\n",
      "119    0\n",
      "120    0\n",
      "Name: income, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create X dataframe (features)\n",
    "X = adult_dt.drop('income', axis=1)\n",
    "\n",
    "# Create Y dataframe (target)\n",
    "Y = adult_dt['income']\n",
    "\n",
    "# Display X and Y shapes to verify\n",
    "print('Shape of X:', X.shape)\n",
    "print('Shape of Y:', Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (22792, 14)\n",
      "Shape of X_test: (9769, 14)\n",
      "Shape of Y_train: (22792,)\n",
      "Shape of Y_test: (9769,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets (70-30 split, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting splits\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of Y_train:', Y_train.shape)\n",
    "print('Shape of Y_test:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random state is key to producing reproducible results, as it ensures that the dataset is split in the same way into training and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_transformed_df shape: (22792, 100)\n",
      "X_test_transformed_df shape: (9769, 98)\n",
      "        age    fnlwgt education-num capital-gain capital-loss hours-per-week  \\\n",
      "0 -0.334771 -1.261292     -0.423219    -0.146349     5.136118      -2.309851   \n",
      "1  0.691223  0.510642     -0.035377     0.929742    -0.219921      -0.031295   \n",
      "2 -1.140909 -1.261841     -0.035377    -0.146349    -0.219921      -0.845065   \n",
      "3  1.277505  0.622115     -0.423219    -0.146349    -0.219921      -0.031295   \n",
      "4 -1.580621  1.018212     -1.198902    -0.146349    -0.219921      -2.553982   \n",
      "\n",
      "   workclass_ Federal-gov  workclass_ Local-gov  workclass_ Never-worked  \\\n",
      "0                   False                 False                    False   \n",
      "1                   False                 False                    False   \n",
      "2                   False                 False                    False   \n",
      "3                   False                  True                    False   \n",
      "4                   False                 False                    False   \n",
      "\n",
      "   workclass_ Private  ...  native-country_ Portugal  \\\n",
      "0               False  ...                     False   \n",
      "1               False  ...                     False   \n",
      "2               False  ...                     False   \n",
      "3               False  ...                     False   \n",
      "4                True  ...                     False   \n",
      "\n",
      "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
      "0                        False                     False   \n",
      "1                        False                     False   \n",
      "2                        False                     False   \n",
      "3                        False                     False   \n",
      "4                        False                     False   \n",
      "\n",
      "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
      "0                  False                   False                     False   \n",
      "1                  False                   False                     False   \n",
      "2                  False                   False                     False   \n",
      "3                  False                   False                     False   \n",
      "4                  False                   False                     False   \n",
      "\n",
      "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
      "0                            False                           True   \n",
      "1                            False                           True   \n",
      "2                            False                           True   \n",
      "3                            False                           True   \n",
      "4                            False                           True   \n",
      "\n",
      "   native-country_ Vietnam  native-country_ Yugoslavia  \n",
      "0                    False                       False  \n",
      "1                    False                       False  \n",
      "2                    False                       False  \n",
      "3                    False                       False  \n",
      "4                    False                       False  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "        age    fnlwgt education-num capital-gain capital-loss hours-per-week  \\\n",
      "0 -0.847768 -0.279401     -0.035377    -0.146349    -0.219921      -0.194049   \n",
      "1  0.471367  -1.31708     -0.423219    -0.146349    -0.219921      -0.031295   \n",
      "2 -0.701197 -0.035817      1.128147    -0.146349    -0.219921       1.189361   \n",
      "3 -0.627912    0.0033      1.128147    -0.146349    -0.219921      -0.031295   \n",
      "4 -0.701197  -0.00327     -0.035377     0.161865    -0.219921       0.782475   \n",
      "\n",
      "   workclass_ Federal-gov  workclass_ Local-gov  workclass_ Private  \\\n",
      "0                   False                 False                True   \n",
      "1                   False                 False               False   \n",
      "2                   False                 False                True   \n",
      "3                   False                 False                True   \n",
      "4                   False                 False               False   \n",
      "\n",
      "   workclass_ Self-emp-inc  ...  native-country_ Portugal  \\\n",
      "0                    False  ...                     False   \n",
      "1                    False  ...                     False   \n",
      "2                    False  ...                     False   \n",
      "3                    False  ...                     False   \n",
      "4                    False  ...                     False   \n",
      "\n",
      "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
      "0                        False                     False   \n",
      "1                        False                     False   \n",
      "2                        False                     False   \n",
      "3                        False                     False   \n",
      "4                        False                     False   \n",
      "\n",
      "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
      "0                  False                   False                     False   \n",
      "1                  False                   False                     False   \n",
      "2                  False                   False                     False   \n",
      "3                  False                   False                     False   \n",
      "4                  False                   False                     False   \n",
      "\n",
      "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
      "0                            False                           True   \n",
      "1                            False                           True   \n",
      "2                            False                           True   \n",
      "3                            False                           True   \n",
      "4                            False                           True   \n",
      "\n",
      "   native-country_ Vietnam  native-country_ Yugoslavia  \n",
      "0                    False                       False  \n",
      "1                    False                       False  \n",
      "2                    False                       False  \n",
      "3                    False                       False  \n",
      "4                    False                       False  \n",
      "\n",
      "[5 rows x 98 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and X_test are your feature DataFrames\n",
    "\n",
    "# Numerical features: columns that are not categorical\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Categorical features: columns that are 'object' type\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Custom function to apply pd.get_dummies to categorical columns\n",
    "def pd_get_dummies(df, columns):\n",
    "    return pd.get_dummies(df, columns=columns, drop_first=True)\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', 'passthrough', categorical_features)  # Use 'passthrough' to keep categorical columns unchanged\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert transformed data to DataFrame\n",
    "# For X_train_transformed\n",
    "X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=list(numerical_features) + list(categorical_features))\n",
    "# For X_test_transformed\n",
    "X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=list(numerical_features) + list(categorical_features))\n",
    "\n",
    "# Apply pd.get_dummies to categorical columns in the transformed DataFrames\n",
    "X_train_transformed_df = pd_get_dummies(X_train_transformed_df, columns=categorical_features)\n",
    "X_test_transformed_df = pd_get_dummies(X_test_transformed_df, columns=categorical_features)\n",
    "\n",
    "# Display the shapes for verification\n",
    "print(\"X_train_transformed_df shape:\", X_train_transformed_df.shape)\n",
    "print(\"X_test_transformed_df shape:\", X_test_transformed_df.shape)\n",
    "\n",
    "# Display the transformed data\n",
    "print(X_train_transformed_df.head())\n",
    "print(X_test_transformed_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8548469648889344\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      7455\n",
      "           1       0.72      0.63      0.67      2314\n",
      "\n",
      "    accuracy                           0.85      9769\n",
      "   macro avg       0.81      0.78      0.79      9769\n",
      "weighted avg       0.85      0.85      0.85      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and X_test are your feature DataFrames\n",
    "# Assuming Y_train and Y_test are your target Series or arrays\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features using OneHotEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # OneHotEncoder for categorical variables\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline combining preprocessing and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsi_participant/lib/python3.9/site-packages/sklearn/metrics/_scorer.py:548: FutureWarning: The `needs_threshold` and `needs_proba` parameter are deprecated in version 1.4 and will be removed in 1.6. You can either let `response_method` be `None` or set it to `predict` to preserve the same behaviour.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Loss (mean): 0.3535968297855201\n",
      "ROC AUC (mean): 0.9037924780402848\n",
      "Accuracy (mean): 0.8520098391129668\n",
      "Balanced Accuracy (mean): 0.7735260515158988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, balanced_accuracy_score, roc_auc_score, log_loss\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and X_test are your feature DataFrames\n",
    "# Assuming Y_train and Y_test are your target Series or arrays\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features using OneHotEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # OneHotEncoder for categorical variables\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline combining preprocessing and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Define metrics to measure during cross-validation\n",
    "scoring = {\n",
    "    'log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': make_scorer(balanced_accuracy_score)\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(pipeline, X_train, Y_train, scoring=scoring, cv=5, n_jobs=-1)\n",
    "\n",
    "# Extract and print results\n",
    "print(\"Negative Log Loss (mean):\", -cv_results['test_log_loss'].mean())\n",
    "print(\"ROC AUC (mean):\", cv_results['test_roc_auc'].mean())\n",
    "print(\"Accuracy (mean):\", cv_results['test_accuracy'].mean())\n",
    "print(\"Balanced Accuracy (mean):\", cv_results['test_balanced_accuracy'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold-level results sorted by Negative Log Loss:\n",
      "   Fold  Negative Log Loss   ROC AUC  Accuracy  Balanced Accuracy\n",
      "4     5           0.345979  0.902568  0.854761           0.775220\n",
      "0     1           0.348716  0.905604  0.851064           0.776618\n",
      "1     2           0.355970  0.901597  0.844703           0.765044\n",
      "3     4           0.357709  0.906501  0.856955           0.777899\n",
      "2     3           0.359610  0.902692  0.852567           0.772849\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'Negative Log Loss': -cv_results['test_log_loss'],\n",
    "    'ROC AUC': cv_results['test_roc_auc'],\n",
    "    'Accuracy': cv_results['test_accuracy'],\n",
    "    'Balanced Accuracy': cv_results['test_balanced_accuracy']\n",
    "})\n",
    "\n",
    "# Sort results by Negative Log Loss\n",
    "results_df_sorted = results_df.sort_values(by='Negative Log Loss')\n",
    "\n",
    "# Display sorted results\n",
    "print(\"Fold-level results sorted by Negative Log Loss:\")\n",
    "print(results_df_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_metrics = results_df.mean()\n",
    "\n",
    "# Display mean metrics\n",
    "print(\"Mean metrics across all folds:\")\n",
    "print(mean_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Loss: 0.3683\n",
      "ROC AUC: 0.9002\n",
      "Accuracy: 0.8551\n",
      "Balanced Accuracy: 0.7763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_train and X_test are your feature DataFrames\n",
    "# Assuming Y_train and Y_test are your target Series or arrays\n",
    "\n",
    "# Define numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='distance')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features using OneHotEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # OneHotEncoder for categorical variables\n",
    "])\n",
    "\n",
    "# Combine transformers using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Pipeline combining preprocessing and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predict probabilities on the test set\n",
    "Y_pred_proba = pipeline.predict_proba(X_test)\n",
    "\n",
    "# Calculate negative log loss\n",
    "logloss = log_loss(Y_test, Y_pred_proba)\n",
    "\n",
    "# Calculate ROC AUC\n",
    "roc_auc = roc_auc_score(Y_test, Y_pred_proba[:, 1])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, pipeline.predict(X_test))\n",
    "\n",
    "# Calculate balanced accuracy\n",
    "balanced_accuracy = balanced_accuracy_score(Y_test, pipeline.predict(X_test))\n",
    "\n",
    "# Print the results\n",
    "print(f\"Negative Log Loss: {logloss:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recoded because it's easier to treat income as a class, since incomes could have many unique values for such a large dataset. It's simpler therefore, to treat adults who make more than 50K a year as \"high-income\" class and the others as a base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "|Criteria|Complete|Incomplete|\n",
    "|---------------------|----|----|\n",
    "|Evaluation of model pipeline |Model pipeline was evaluated correctly.|Model pipeline was not evaluated correctly.|\n",
    "|Explanation of answer|Answer was concise and explained the learner's reasoning in depth.|Answer was not concise and did not explained the learner's reasoning in depth.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
